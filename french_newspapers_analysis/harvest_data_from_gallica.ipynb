{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports#\n",
    "import os\n",
    "import time\n",
    "#from gallipy import Resource, Ark\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create directory to save newspapers from gallica \n",
    "HERE = _dh[-1]\n",
    "OUT_DIR = Path(HERE) / \"gallica_data\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cds= { \n",
    "      \"L'Action française\": \"cb326819451_date\",\n",
    "      \"L'Aurore\": \"cb32706846t_date\",\n",
    "      \"Le Constitutionnel\": \"cb32747578p_date\",\n",
    "      \"La Croix\": \"cb343631418_date\",\n",
    "      \"Figaro : journal non politique\": \"cb34355551z_date\",\n",
    "      \"Le Populaire\": \"cb34393339w_date\",\n",
    "      \"L'Humanité\": \"cb327877302_date\",\n",
    "      \"Le Temps\": \"cb34431794k_date\",\n",
    "      \"Le Petit Journal\": \"cb32895690j_date\",\n",
    "      \"Le Petit Parisien\": \"cb34419111x_date\",\n",
    "      \"La Justice\": \"cb32802914p_date\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_titles= { \n",
    "      \"L'Action française\": \"L_Action_francaise\",\n",
    "      \"L'Aurore\": \"L_Aurore\",\n",
    "      \"Le Constitutionnel\": \"Le_Constitutionnel\",\n",
    "      \"La Croix\": \"La_Croix\",\n",
    "      \"Figaro : journal non politique\": \"Le_Figaro\",\n",
    "      \"Le Populaire\": \"Le_Populaire\",\n",
    "      \"L'Humanité\": \"L_Humanite\",\n",
    "      \"Le Temps\": \"Le_Temps\",\n",
    "      \"Le Petit Journal\": \"Le_Petit_Journal\",\n",
    "      \"Le Petit Parisien\": \"Le_Petit_Parisien\",\n",
    "      \"La Justice\": \"La_Justice\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLES = [\n",
    "    \"L'Action française\",\n",
    "    \"L'Aurore\",\n",
    "    \"Le Constitutionnel\",\n",
    "    \"La Croix\",\n",
    "    \"Figaro : journal non politique\",\n",
    "    \"Le Populaire\",\n",
    "    \"L'Humanité\",\n",
    "    \"Le Temps\",\n",
    "    \"Le Petit Journal\",\n",
    "    \"Le Petit Parisien\",\n",
    "    \"La Justice\",\n",
    "    \n",
    "]\n",
    "title_part = \" OR \".join(f'dc.title adj \"{t}\"' for t in TITLES)\n",
    "\n",
    "CQL = (\n",
    "    f'({title_part}) '\n",
    "    'and dc.type all \"fascicule\" and dc.date adj \"19140218\"'\n",
    ")\n",
    "def create_cql(journal, year) -> str:\n",
    "    cd = cds[journal]\n",
    "    return (\n",
    "        \n",
    "        f'dc.title adj \"{journal}\"'\n",
    "        \n",
    "        f' and (dc.date = \"{year}\")'\n",
    "        f' and arkPress all \"{cds[journal]}\"'\n",
    "    )\n",
    "HITS_PER_PAGE = 15\n",
    "\n",
    "PAUSE_BETWEEN_HITS = 60.0    \n",
    "PAUSE_BETWEEN_ISSUES = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_YEAR = {\n",
    "    \"L'Action française\": 1908,\n",
    "    \"L'Aurore\":            1897,\n",
    "    \"Le Constitutionnel\":  1870,\n",
    "    \"La Croix\":            1880,\n",
    "    \"Figaro : journal non politique\":           1870,\n",
    "    \"Le Populaire\":        1918,\n",
    "    \"L'Humanité\":          1904,\n",
    "    \"Le Temps\":            1870,\n",
    "    \"Le Petit Journal\":     1870,\n",
    "}\n",
    "\n",
    "LAST_YEAR = {\n",
    "    \"L'Action française\": 1940,   # banned 1944, but we harvest ≤ 1940\n",
    "    \"L'Aurore\":           1940,   # continues later, cap at 1940\n",
    "    \"Le Constitutionnel\": 1914,\n",
    "    \"La Croix\":           1940,   # still published; cap at 1940\n",
    "    \"Figaro : journal non politique\":          1940,\n",
    "    \"Le Populaire\":       1940,\n",
    "    \"L'Humanité\":         1939,\n",
    "    \"Le Temps\":           1940,   # actually ceases 1942; cap at 1940\n",
    "    \"Le Petit Journal\":   1940,   # ceases 1944; cap at 1940\n",
    "   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gallica endpoints\n",
    "SRU_URL = \"https://gallica.bnf.fr/SRU\"\n",
    "DOC_URL = \"https://gallica.bnf.fr/services/Document\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#helpers\n",
    "\n",
    "def run_sru_page(start_record: int, title, year):\n",
    "    print(create_cql(title, year))\n",
    "    params = {\n",
    "        \"version\": \"1.2\",\n",
    "        \"operation\": \"searchRetrieve\",\n",
    "        \"query\": create_cql(title, year),\n",
    "\n",
    "        \"startRecord\": start_record,\n",
    "        \"maximumRecords\": HITS_PER_PAGE,\n",
    "        \"recordSchema\": \"dc\",\n",
    "        \"collapsing\": \"false\"      # ← key line: get *every* fascicule\n",
    "    }\n",
    "    \n",
    "    resp = requests.get(SRU_URL, params=params)\n",
    "    resp.raise_for_status()\n",
    "    xml_text = resp.text\n",
    "    root = ET.fromstring(xml_text)\n",
    "    ns = {\"srw\": \"http://www.loc.gov/zing/srw/\"}\n",
    "    num_elem = root.find(\"./srw:numberOfRecords\", ns)\n",
    "    total_hits = int(num_elem.text) if num_elem is not None else 0\n",
    "\n",
    "    return xml_text, total_hits\n",
    "\n",
    "def extract_arks_from_sru(xml_text: str):\n",
    "    ns = {\n",
    "        \"srw\": \"http://www.loc.gov/zing/srw/\",\n",
    "    }\n",
    "    root = ET.fromstring(xml_text)\n",
    "    arks = []\n",
    "    print(f\"Found {len(root.findall('.//srw:record', ns))} records in SRU response.\", root)\n",
    "\n",
    "    for record in root.findall(\".//srw:record\", ns):\n",
    "        uri_elem = record.find(\".//srw:extraRecordData/uri\", ns)\n",
    "        if uri_elem is not None:\n",
    "            obj_id = uri_elem.text.strip()  \n",
    "            arks.append(f\"ark:/12148/{obj_id}\")\n",
    "    return arks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, time, requests, pathlib, urllib3\n",
    "from requests.exceptions import ConnectionError, ReadTimeout\n",
    "\n",
    "UA = \"GallicaHarvester/0.3 (mara00008@stud.uni-saarland.de)\"\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\"User-Agent\": UA})\n",
    "\n",
    "def safe_get(url: str,\n",
    "             tries: int = 6,\n",
    "             base_wait: float = 10.0,\n",
    "             max_wait: float = 120.0) -> str:\n",
    "    \n",
    "    for attempt in range(1, tries + 1):\n",
    "        try:\n",
    "            r = SESSION.get(url, timeout=30)\n",
    "        except (ConnectionError, ReadTimeout) as e:\n",
    "            wait = min(max_wait, base_wait * 2 ** (attempt - 1))\n",
    "            wait += random.uniform(3, 12)\n",
    "            print(f\"[{attempt}/{tries}] connection problem \"\n",
    "                  f\"({e.__class__.__name__}), sleep {wait:.1f}s\")\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "\n",
    "        # HTTP response received:\n",
    "        if r.status_code == 200 and b\"denied\" not in r.content[:400]:\n",
    "            return r.text\n",
    "\n",
    "        # otherwise it is 429 “Too many requests” or 503 “Service unavailable”\n",
    "        retry_after = r.headers.get(\"Retry-After\")\n",
    "        wait = int(retry_after or 0)\n",
    "        wait = max(wait, base_wait * 2 ** (attempt - 1))\n",
    "        wait = min(wait, max_wait)\n",
    "        wait += random.uniform(3, 12)\n",
    "        print(f\"[{attempt}/{tries}] {r.status_code} blocked, \"\n",
    "              f\"sleep {wait:.1f}s (Retry-After={retry_after})\")\n",
    "        time.sleep(wait)\n",
    "\n",
    "    # All tries exhausted\n",
    "    raise RuntimeError(f\"safe_get(): gave up after {tries} attempts → {url}\")\n",
    "\n",
    "def download_issue_pdf(ark, outdir):\n",
    "    out = outdir / f\"{ark.split('/')[-1]}.txt\"\n",
    "    if out.exists():\n",
    "        return False            # don’t hit the server again\n",
    "    url = f\"https://gallica.bnf.fr/{ark}/f1n4.texteBrut\"\n",
    "    txt = safe_get(url)\n",
    "    out.write_text(txt, encoding=\"utf-8\")\n",
    "    # polite pause before next OCR page\n",
    "    time.sleep(random.uniform(2, 8))\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harvest_all_press_issues():\n",
    "    for journal in TITLES:\n",
    "        jdir = OUT_DIR / repo_titles[journal]\n",
    "        jdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for year in range(FIRST_YEAR[journal], LAST_YEAR[journal] + 1):\n",
    "            print(f\"\\nHarvesting {journal} – {year}\")\n",
    "            ydir = jdir / str(year)\n",
    "            ydir.mkdir(exist_ok=True)\n",
    "\n",
    "            page = 1                 \n",
    "            downloaded = 0\n",
    "\n",
    "            while True:\n",
    "                start_record = (page - 1) * HITS_PER_PAGE + 1\n",
    "                xml, total = run_sru_page(start_record, journal, year)\n",
    "\n",
    "                if page == 1:\n",
    "                    print(f\"Gallica reports {total} issues\")\n",
    "\n",
    "                arks = extract_arks_from_sru(xml)\n",
    "                if not arks:\n",
    "                    print(\"No more ARKs -> stop year\")\n",
    "                    break\n",
    "\n",
    "                print(f\"  SRU page {page}: {len(arks)} ARKs\")\n",
    "                for ark in arks:\n",
    "                    if download_issue_pdf(ark, ydir):   \n",
    "                        downloaded += 1\n",
    "\n",
    "                # Have we reached / passed the last block \n",
    "                if page * HITS_PER_PAGE >= total:\n",
    "                    break\n",
    "\n",
    "                page += 1\n",
    "                time.sleep(PAUSE_BETWEEN_HITS)\n",
    "\n",
    "            print(f\"Finished {year}: {downloaded}/{total} issues saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Harvesting Le Petit Parisien – 1876\n",
      "dc.title adj \"Le Petit Parisien\" and (dc.date = \"1876\") and arkPress all \"cb34419111x_date\"\n",
      "Gallica reports 75 issues\n",
      "Found 15 records in SRU response. <Element '{http://www.loc.gov/zing/srw/}searchRetrieveResponse' at 0x7f8ca4bd7ec0>\n",
      "  SRU page 1: 15 ARKs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dc.title adj \"Le Petit Parisien\" and (dc.date = \"1876\") and arkPress all \"cb34419111x_date\"\n",
      "Found 15 records in SRU response. <Element '{http://www.loc.gov/zing/srw/}searchRetrieveResponse' at 0x7f8ca4bd78d0>\n",
      "  SRU page 2: 15 ARKs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mharvest_all_press_issues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 36\u001b[0m, in \u001b[0;36mharvest_all_press_issues\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     page \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 36\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(PAUSE_BETWEEN_HITS)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdownloaded\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m issues saved\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "harvest_all_press_issues()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsai_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
