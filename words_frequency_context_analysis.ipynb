{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fe79b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, math, collections, itertools, json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from collections import Counter, defaultdict\n",
    "import nltk\n",
    "import csv \n",
    "import csv, re, warnings\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139d5674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/malakrch/DSAI_project/gallica_data')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HERE = _dh[-1]\n",
    "PAPERS_OCRS = HERE / \"gallica_data\"  # Should be downloaded from Gallica and placed in the repo directory\n",
    "OUT_DIR = HERE / \"context_analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58f0ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    FR_STOPWORDS = set(nltk.corpus.stopwords.words('french'))\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    FR_STOPWORDS = set(nltk.corpus.stopwords.words('french'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14256f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "YEAR_START, YEAR_END = 1870, 1940     # years of interest\n",
    "\n",
    "# mapping “journal title” ‑> folder‑name\n",
    "repo_titles = {\n",
    "      \"L'Action française\": \"L_Action_francaise\",\n",
    "      \"L'Aurore\": \"L_Aurore\",\n",
    "      \"Le Constitutionnel\": \"Le_Constitutionnel\",\n",
    "      \"La Croix\": \"La_Croix\",\n",
    "      \"Figaro : journal non politique\": \"Le_Figaro\",\n",
    "      \"Le Populaire\": \"Le_Populaire\",\n",
    "      \"L'Humanité\": \"L_Humanite\",\n",
    "      \"Le Temps\": \"Le_Temps\",\n",
    "      #\"Le Petit Journal\": \"Le_Petit_Journal\",\n",
    "      #\"Le Petit Parisien\": \"Le_Petit_Parisien\",\n",
    "      #\"La Justice\": \"La_Justice\"\n",
    "}\n",
    "\n",
    "\n",
    "ANARCHIST_TOKEN_RE = re.compile(r'anarchist(?:e|es)?|anarchisme', re.IGNORECASE)\n",
    "\n",
    "FAMILY_REGEX = {\n",
    "    'anarchist': re.compile(r'anarchist(?:e|es)?|anarchisme|anarchie', re.IGNORECASE),\n",
    "    'communis'  : re.compile(r'communis\\w*',                   re.IGNORECASE),\n",
    "    'socialis'  : re.compile(r'socialis\\w*',                   re.IGNORECASE),\n",
    "    'revolution': re.compile(r'r[eé]volution\\w*',              re.IGNORECASE),\n",
    "}\n",
    "\n",
    "WINDOW_SIZE = 5     #number tokens for context windows\n",
    "TOKEN_RE = re.compile(r'[a-zàâçéèêëîïôûùüÿñæœ]+', re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86a1fd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_tokens(tokens, w=5):\n",
    "    \"\"\"Yield context tokens around every anarchist hit (stop-words removed).\"\"\"\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if ANARCHIST_TOKEN_RE.fullmatch(tok):\n",
    "            left  = max(0, i - w)\n",
    "            right = min(len(tokens), i + w + 1)\n",
    "            for t in tokens[left:right]:\n",
    "                if t != tok and t not in FR_STOPWORDS:\n",
    "                    yield t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a33e249",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_counts       = Counter()                                \n",
    "ctx_counts          = defaultdict(lambda: defaultdict(Counter))   \n",
    "anarchist_hit_counts= defaultdict(lambda: defaultdict(int))       \n",
    "\n",
    "for title, folder in repo_titles.items():\n",
    "    paper_dir = PAPERS_OCRS / folder\n",
    "    if not paper_dir.exists():\n",
    "        warnings.warn(f\"Missing directory {paper_dir}\")\n",
    "        continue\n",
    "\n",
    "    for year in range(YEAR_START, YEAR_END + 1):\n",
    "        year_dir = paper_dir / str(year)\n",
    "        if not year_dir.exists():\n",
    "            continue\n",
    "\n",
    "        for path in year_dir.glob(\"*.txt\"):\n",
    "            try:\n",
    "                txt = path.read_text(encoding=\"utf-8\", errors=\"ignore\").lower()\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"Could not read {path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            tokens = TOKEN_RE.findall(txt)\n",
    "\n",
    "            #total word count for the year\n",
    "            global_counts.update(tokens)\n",
    "\n",
    "            #anarchist hits frequency\n",
    "            a_hits = sum(1 for t in tokens if FAMILY_REGEX['anarchist'].fullmatch(t))\n",
    "            if a_hits:\n",
    "                anarchist_hit_counts[title][year] += a_hits\n",
    "                \n",
    "                #words found in context of anarchist hits\n",
    "                ctx_counts[title][year].update(window_tokens(tokens))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af396c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = sum(global_counts.values())\n",
    "\n",
    "out_path = OUT_DIR / \"pmi_context_words_by_paper_year.csv\"\n",
    "with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as fout:\n",
    "    writer = csv.writer(fout)\n",
    "    writer.writerow([\"paper\",\"year\",\"word\",\"ctx_freq\",\"PMI\"])\n",
    "\n",
    "    for paper, year_counters in ctx_counts.items():\n",
    "        for year, counter in year_counters.items():\n",
    "            c_x = anarchist_hit_counts[paper][year]         \n",
    "            if c_x == 0:\n",
    "                continue\n",
    "\n",
    "            for word, c_xy in counter.items():\n",
    "                c_y = global_counts[word]\n",
    "                if c_y < 5 or c_xy < 3:                      # skip very rare words\n",
    "                    continue\n",
    "                pmi = math.log2((c_xy * N) / (c_x * c_y))\n",
    "                writer.writerow([paper, year, word, c_xy, round(pmi, 3)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11d6754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             paper  year       word  ctx_freq    PMI\n",
      "0   Figaro : journal non politique  1871          p         4  3.495\n",
      "1   Figaro : journal non politique  1871  slategray         4  1.783\n",
      "2   Figaro : journal non politique  1871      color         4  1.783\n",
      "3   Figaro : journal non politique  1871       span         7  1.590\n",
      "4   Figaro : journal non politique  1871      style         3  1.368\n",
      "5   Figaro : journal non politique  1872  slategray         4  2.783\n",
      "6   Figaro : journal non politique  1872       span         7  2.590\n",
      "7   Figaro : journal non politique  1872      style         3  2.368\n",
      "8   Figaro : journal non politique  1872      color         3  2.368\n",
      "9   Figaro : journal non politique  1874       span         3  2.368\n",
      "10  Figaro : journal non politique  1876  slategray         5  2.882\n",
      "11  Figaro : journal non politique  1876       span         8  2.560\n",
      "12  Figaro : journal non politique  1876      style         4  2.560\n",
      "13  Figaro : journal non politique  1876      color         4  2.560\n",
      "14  Figaro : journal non politique  1878       span         4  1.046\n",
      "15  Figaro : journal non politique  1879       span         6  2.953\n",
      "16  Figaro : journal non politique  1879      style         3  2.953\n",
      "17  Figaro : journal non politique  1879      color         3  2.953\n",
      "18  Figaro : journal non politique  1879  slategray         3  2.953\n",
      "19  Figaro : journal non politique  1880          p         9  4.343\n"
     ]
    }
   ],
   "source": [
    "full = pd.read_csv(OUT_DIR/\"pmi_context_words_by_paper_year.csv\")\n",
    "\n",
    "#take the 10 rows with the largest PMI inside every (paper, year) group\n",
    "top10 = (\n",
    "    full.sort_values([\"paper\", \"year\", \"PMI\"], ascending=[True, True, False])\n",
    "        .groupby([\"paper\", \"year\"], as_index=False)\n",
    "        .head(10)               \n",
    "        .reset_index(drop=True) \n",
    ")\n",
    "\n",
    "top10.to_csv(OUT_DIR / \"top10_context_words_by_paper_year.csv\", index=False)\n",
    "\n",
    "print(top10.head(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee9644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for title, folder in repo_titles.items():\n",
    "    paper_dir = PAPERS_OCRS / folder\n",
    "    if not paper_dir.exists():\n",
    "        warnings.warn(f\"Missing directory: {paper_dir}\")\n",
    "        continue\n",
    "\n",
    "    out_path  = HERE / \"context_analysis\" / f\"{folder}_frequencies.csv\"\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as fout:\n",
    "        writer = csv.writer(fout)\n",
    "        writer.writerow([\n",
    "            \"paper\", \"year\",\n",
    "            \"anarchist_hits\", \"communis_hits\",\n",
    "            \"socialis_hits\", \"revolution_hits\"\n",
    "        ])\n",
    "\n",
    "        for year in range(YEAR_START, YEAR_END + 1):\n",
    "            year_dir = paper_dir / str(year)\n",
    "            if not year_dir.exists():\n",
    "                continue\n",
    "\n",
    "            for path in year_dir.glob(\"*.txt\"):\n",
    "                try:\n",
    "                    txt = path.read_text(encoding=\"utf-8\", errors=\"ignore\").lower()\n",
    "                except Exception as e:\n",
    "                    warnings.warn(f\"Could not read {path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                \n",
    "                counts = {\n",
    "                    fam: len(rx.findall(txt))\n",
    "                    for fam, rx in FAMILY_REGEX.items()\n",
    "                }\n",
    "\n",
    "                writer.writerow([\n",
    "                    title, year,\n",
    "                    counts[\"anarchist\"],\n",
    "                    counts[\"communis\"],\n",
    "                    counts[\"socialis\"],\n",
    "                    counts[\"revolution\"],\n",
    "                ])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsai_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
